{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c1900c",
   "metadata": {},
   "source": [
    "# LangChain: Models, Prompts and Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c371518f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial from DeepLearning.AI shows us how we can harness the power of OpenAI's GPT-3.5 Turbo model through the utilization of models, prompts, and parsers.\n",
    "\n",
    "In the next few sections, we will explore how these three essential components work together to create intelligent applications. \n",
    "\n",
    "By understanding models, crafting effective prompts, and leveraging LangChain abstractions, we'll gain the skills to build versatile and responsive applications powered by state-of-the-art language processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef5a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc27fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keys.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "openai.api_key = lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d292e47",
   "metadata": {},
   "source": [
    "## Chat API : OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cbfaad",
   "metadata": {},
   "source": [
    "Let's start with a direct API calls to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a79c27a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper function for prompt output\n",
    "def get_completion(prompt, model='gpt-3.5-turbo'):\n",
    "    messages = [{'role':'user', 'content':prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = 0,)\n",
    "    return response.choices[0].message['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e393c6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The founders of OpenAI are Elon Musk, Sam Altman, Greg Brockman, Ilya Sutskever, John Schulman, and Wojciech Zaremba.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion('Who are the founders of OpenAI?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154e245",
   "metadata": {},
   "source": [
    "### Example with email response: Complaint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97677d73",
   "metadata": {},
   "source": [
    "Below we have a customer complaint that is written in 'Pirate' English. We'll convert them into two different versions: American English, and King James English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c2975d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer email with complaint in the Pirate English\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b6231",
   "metadata": {},
   "source": [
    "Define the styles we want to apply to our email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc259f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_1 = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a4ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_2 = \"\"\"King James English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e19434b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Translate the text that is delimited by triple backmarks into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ``` \n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = f\"\"\" Translate the text \\\n",
    "that is delimited by triple backmarks into a style that is {style_1}.\n",
    "text: ``` {customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd1e7aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Translate the text that is delimited by triple backmarks into a style that is King James English in a calm and respectful tone\n",
      ".\n",
      "text: ``` \n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = f\"\"\" Translate the text \\\n",
    "that is delimited by triple backmarks into a style that is {style_2}.\n",
    "text: ``` {customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93ff8e",
   "metadata": {},
   "source": [
    "Outputs of our email complaints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "204e7f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! And to make things even worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, my friend!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_1 = get_completion(prompt_1)\n",
    "response_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94ee11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Verily, I am vexed that mine blender lid hath flown off and bespattered mine kitchen walls with smoothie! And to compound mine troubles, the warranty doth not extend to the expense of cleansing mine kitchen. I do beseech thy aid forthwith, good sir!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_2 = get_completion(prompt_2)\n",
    "response_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3282bc8d",
   "metadata": {},
   "source": [
    "## Chat API : LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40846652",
   "metadata": {},
   "source": [
    "Let's see how we can do the same using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "870fbeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f93e06",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a852b",
   "metadata": {},
   "source": [
    "We know that we can control the randomness and creativity of the generated text by tweaking the `temperature`. We'll initiate 2 models with identical paramters, other than their temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07ebe69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less random model: Set temperature = 0.0\n",
    "chat = ChatOpenAI(openai_api_key = openai.api_key, temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8586075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More random model: Set temperature = 0.8\n",
    "chat_hi_temp = ChatOpenAI(openai_api_key = openai.api_key, temperature = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd321c",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b8474e",
   "metadata": {},
   "source": [
    "Prompts can be long and detailed. By using prompt templates, we can save time when we've constructed an effective one that meets our purposes.\n",
    "\n",
    "We'll go through an example of how we store a template using `ChatPromptTemplate`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f399d7",
   "metadata": {},
   "source": [
    "We start off with are creating a prompt template, `template_string`, that asks the LLM to translate an input `text` into a certain `style` that we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ca2c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456fafd8",
   "metadata": {},
   "source": [
    "Import `ChatPromptTemplate` to re-use the prompt template above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8427ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['text', 'style'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True), additional_kwargs={})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Feed the template_string in as the argument\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667384eb",
   "metadata": {},
   "source": [
    "When we extract the `prompt`, we see the 2 input variables, `style` and `text`, that we included in our prompt earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90e484c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3b624d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ed4f5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f46d2",
   "metadata": {},
   "source": [
    "### Example with email response: Customer Complaint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3f9288",
   "metadata": {},
   "source": [
    "We'll resue the same `customer_style` and `customer_email` from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e49ed82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef037724",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6cd7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e3ac0d",
   "metadata": {},
   "source": [
    "Notice that the output is a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87941cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain.schema.messages.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1549d8",
   "metadata": {},
   "source": [
    "The first element of the list is the prompt, `template_string`, we fed it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cb93a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_messages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d607a1",
   "metadata": {},
   "source": [
    "Let's feed our customer message into the two models we initiated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3f846b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! And to make things even worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, my friend!\n"
     ]
    }
   ],
   "source": [
    "customer_response = chat(customer_messages)\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a979d700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am quite frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! To add to the difficulties, the warranty does not cover the expenses for cleaning up my kitchen. I would greatly appreciate your assistance at this moment, my friend.\n"
     ]
    }
   ],
   "source": [
    "customer_response_hi_temp = chat_hi_temp(customer_messages)\n",
    "print(customer_response_hi_temp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26a9245",
   "metadata": {},
   "source": [
    "The output we get is a translation to polite American English. It appears that there isn't much difference. Perhaps this is because the email input is short and clear, leaving little room for much variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926029cf",
   "metadata": {},
   "source": [
    "### Example with email response: Customer Service Reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91dfe48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2aebe4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_style = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in King James English, \\\n",
    "with undertones of pomposity \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2254525d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in King James English, with undertones of pomposity \n",
      ". text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f4380c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hearken, goodly customer, verily I say unto thee, the warranty doth not extend its benevolent protection to the expenses incurred in the cleansing of thy kitchen, forsooth! For it is thy very own folly that hath led thee astray, in thy misuse of the blender, by neglecting to place the lid thereupon ere commencing its operation. Alas, thou art left to thy own devices! Fare thee well, and mayhap we shall meet again anon!\n"
     ]
    }
   ],
   "source": [
    "service_response = chat(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91bdccf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harken, my goodly patron, disquiet thyself not, forsooth, for it is with a heavy heart that I must impart upon thee this grave tidings. Verily, the warranty, that sacred covenant betwixt us, doth not extend its benevolence unto the expenses of cleaning thy kitchens. Alas, the fault lies not upon the divine essence of thy blender, but upon thine own transgressions, whereby thou didst foolishly neglect to secure the lid ere commencing the blending process. 'Tis a cruel twist of fate, I warrant thee! Farewell and adieu, dear compatriot!\n"
     ]
    }
   ],
   "source": [
    "service_response_hi_temp = chat_hi_temp(service_messages)\n",
    "print(service_response_hi_temp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850b5b9",
   "metadata": {},
   "source": [
    "There is a more noticeable difference in the model outputs when we request for a translation in King James English. This is probably due to it's more expressive and colourful nature, which allows the LLM more room to play around with words.\n",
    "\n",
    "Another possible reason is that LLMs are mostly trained with American English. And as their learning is reinforced by human feedback, they've become fine-tuned to yield a standardized English style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aea8b3",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774630b2",
   "metadata": {},
   "source": [
    "To make our output more readable, we can define the format that we want the LLM's outputs in.\n",
    "\n",
    "Let's start with defining how we want our information from the LLM to be formatted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "147a9b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df46a3d",
   "metadata": {},
   "source": [
    "Below we have a sample customer review, `customer_review`, as well as a target template, `review_template`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59111992",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d820a5",
   "metadata": {},
   "source": [
    "Firstly, our desired output format, `review_template`, asks the LLM to take the customer review as input.\n",
    "\n",
    "Next, it asks the LLM to extract three fields from it and then format the output as JSON with the following keys:\n",
    "- `gift`\n",
    "- `delivery_days`\n",
    "- `price_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28890e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fcf5bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n', template_format='f-string', validate_template=True), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08a8630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gift\": false,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "chat = ChatOpenAI(temperature=0.0, openai_api_key = openai.api_key)\n",
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1442d7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbce153",
   "metadata": {},
   "source": [
    "Although the response appears like a JSON format with key-value pairs, the output is actually just one long string.\n",
    "\n",
    "Hence, we'll get an error by running the line of code below because 'gift' is not a dictionary key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a07d5114",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgift\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "response.content.get('gift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68cc26",
   "metadata": {},
   "source": [
    "### Parse the LLM output string into a Python dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c43a3",
   "metadata": {},
   "source": [
    "Fortunately, LangChain has an output parser that enables us to format our instructions. \n",
    "\n",
    "We start this section by importing `ResponseSchema` and `StructuredOutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c9d6f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce1e2c",
   "metadata": {},
   "source": [
    "Langchain provides the `ResponseSchema` class to help return structured output from agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fec6b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb3e2fc",
   "metadata": {},
   "source": [
    "Next, we create a `StructuredOutputParser` with the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37c15fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e096c7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_schemas=[ResponseSchema(name='gift', description='Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.', type='string'), ResponseSchema(name='delivery_days', description='How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.', type='string'), ResponseSchema(name='price_value', description='Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.', type='string')]\n"
     ]
    }
   ],
   "source": [
    "print(output_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519cab12",
   "metadata": {},
   "source": [
    "We then get `format_instructions` which helps specify the format of of our instruction and the output of the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "430e1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c499b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5547aba3",
   "metadata": {},
   "source": [
    "Below is our new review template. In our previous template, our instruction for formatting the output was this:\n",
    "```\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "```\n",
    "\n",
    "This time, in it's place, we have `format_instructions` defined at the bottom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bdce57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "# Format the prompt\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b1b49",
   "metadata": {},
   "source": [
    "The actual prompt that will be inputted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11ceb3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50ebfef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the OpenAI endpoint with `chat`\n",
    "response = chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae9076",
   "metadata": {},
   "source": [
    "The output from the LLM is a dictionary in JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06073df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": false,\n",
      "\t\"delivery_days\": \"2\",\n",
      "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14615348",
   "metadata": {},
   "source": [
    "LangChain's `output_parser` allows us to extract the values easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58a0e745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False,\n",
       " 'delivery_days': '2',\n",
       " 'price_value': \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7cdaed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8668785a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('delivery_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc61c36",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5547b",
   "metadata": {},
   "source": [
    "This tutorial covered the essential concepts in using LangChain, including models, prompts, and parsers. Models are the language models powering the system, prompts are input instructions for these models, and parsers help structure and extract information from the model's outputs. \n",
    "\n",
    "LangChain simplifies the process of creating reusable prompts and output parsers, making it a valuable tool for building applications with large language models. With these tools at our disposal, we can efficiently harness the power of language models for various tasks, from language translation to data extraction, streamlining our development process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
