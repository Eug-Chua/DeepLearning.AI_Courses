{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74039192",
   "metadata": {},
   "source": [
    "# LangChain: Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b8c3d3",
   "metadata": {},
   "source": [
    "LLM models naturally lack the ability to recall prior conversations.\n",
    "\n",
    "This poses a challenge when developing applications like chatbots that require ongoing interactions. \n",
    "\n",
    "In this tutorial, we will delve into the concept of memory, which enables LLMs to retain past conversation segments. This stored information is then utilized to maintain a seamless conversational flow during interactions. \n",
    "\n",
    "Below, we'll explore how LangChain provides various advanced mechanisms for managing these memories below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f2a41",
   "metadata": {},
   "source": [
    "## Import Libaries and Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9e9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cfaf2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keys.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "openai.api_key = lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5380e49",
   "metadata": {},
   "source": [
    "## Langchain's ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6cbdae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca23a9a",
   "metadata": {},
   "source": [
    "`ConversationBufferMemory` allows us to maintain context and history in conversational agents by storing previous messages in a buffer. It has a `memory_key` parameter that specifies which key in the agent memory will hold the conversation history. By default this is \"chat_history\".\n",
    "\n",
    "As we'll see, this is especially handy when we need to work with conversational agents like ChatOpenAI that need conversation context and history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dca6a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "416bc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0,\n",
    "                 openai_api_key = openai.api_key,\n",
    "                 model=llm_model)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm = llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32162f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Eugene! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='Hi, my name is Eugene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cfb965a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f1c0b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Eugene.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6caa73",
   "metadata": {},
   "source": [
    "By changing the `verbose` argument in `ConversationChain`, we can see LangChain stores the conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec68fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_true = ConversationBufferMemory()\n",
    "\n",
    "conversation_true = ConversationChain(\n",
    "    llm = llm, \n",
    "    memory = memory_true,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0267d857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Eugene\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Eugene! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_true.predict(input='Hi, my name is Eugene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e3fc36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Eugene\n",
      "AI: Hello Eugene! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_true.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a051b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Eugene\n",
      "AI: Hello Eugene! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Eugene.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_true.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959d2e7",
   "metadata": {},
   "source": [
    "The LLM's stores the memory of this conversation with `ConversationBufferMemory()` (which we've assigned to the variable `memory`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef8faef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Eugene\n",
      "AI: Hello Eugene! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Eugene.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "680b52f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Eugene\\nAI: Hello Eugene! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 is equal to 2.\\nHuman: What is my name?\\nAI: Your name is Eugene.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f45a7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "109dac98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Eugene\n",
      "AI: Hello Eugene! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Eugene.\n",
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158181d",
   "metadata": {},
   "source": [
    "We can create a new memory strand by initiating a new instance of `ConversationBufferMemory()`. Let's see how LangChain does it using the example of a simple conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db785730",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b2268c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "422a1ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc4834",
   "metadata": {},
   "source": [
    "Below, we see the variables held in `ConversationBufferMemory()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5493a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb926f",
   "metadata": {},
   "source": [
    "We can add more information to the memory by using the `save_context` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72fb3dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d37d1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc55039f",
   "metadata": {},
   "source": [
    "## LangChain's ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4234a70",
   "metadata": {},
   "source": [
    "When using a large language model for chat conversations, it's important to note that the model itself is stateless.\n",
    "\n",
    "This means it doesn't retain any memory of the ongoing dialogue; each transaction or API call is treated independently. The appearance of memory in chatbots is created through rapid code that supplies the entire conversation history as context to the language model.\n",
    "\n",
    "This memory explicitly stores the previous terms or statements, such as \"Hi, my name is Eugene\" or \"Hello, it's nice to meet you.\" It serves as additional input for the language model, allowing it to generate responses as if it remembers the conversation flow.\n",
    "\n",
    "As conversations grow longer, the memory requirements increase significantly. This can also lead to higher costs, as language models are often priced based on the number of tokens they process.\n",
    "\n",
    "To address these challenges, LangChain offers an alternative memory option to store and accumulate conversations: `ConversationBufferWindowMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29d8a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58a2ae",
   "metadata": {},
   "source": [
    "`ConversationBufferWindowMemory` keeps a list of the interactions of the conversation over time. \n",
    "We can determine the number of interactions we want the LLM to remember through the parameter `k`. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faba58b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6070dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0a99bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c43c9a",
   "metadata": {},
   "source": [
    "Because `k` was set to 1, the memory only holds 1 previous interaction exchange even though though we had 2 exchanges.\n",
    "\n",
    "To further illustrate this, let's re-run our conversation from the section above. But this time, we'll replace `ConversationBufferMemory()` with `ConversationBufferWindowMemory(k=1)` as the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "973d6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0,\n",
    "                 openai_api_key = openai.api_key,\n",
    "                 model=llm_model)\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm = llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fea6160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Eugene! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Eugene\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abf3e59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2efae38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a75871",
   "metadata": {},
   "source": [
    "Notice that the LLM does not have the memory of our first question.\n",
    "\n",
    "Now let's see what is happening to the LLM's memory when we switch `verbose` to `True` and re-run the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3200bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm = llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f7930cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is my name?\n",
      "AI: I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\n",
      "Human: Hi, my name is Eugene\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Eugene! How can I assist you today?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Eugene\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5186d1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Eugene\n",
      "AI: Hello Eugene! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3e29bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1208742d",
   "metadata": {},
   "source": [
    "With each new interaction added to the conversation, the previous memory is cleared - retaining only 1 interaction, the most recent one. This means that at any given moment, only one preceding interaction is stored in memory.\n",
    "\n",
    "While the `ConversationBufferWindowMemory` relies on the number of recent interactions to manage the LLM's memory, we will now explore another type of memory based on token length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71081c",
   "metadata": {},
   "source": [
    "## LangChain's ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19528aa7",
   "metadata": {},
   "source": [
    "The `ConversationTokenBufferMemory` maintains a buffer of recent interactions in memory and uses token length, rather than the number of interactions, as the criteria for deciding when to remove older interactions.\n",
    "\n",
    "Using the `ConversationTokenBufferMemory`, the memory effectively limits the number of tokens used. This approach aligns more closely with LLM pricing, which is often determined by the number of tokens processed during calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "472fbdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, \n",
    "                 openai_api_key=openai.api_key,\n",
    "                 model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6888e7",
   "metadata": {},
   "source": [
    "To illustrate how `ConversationTokenBufferMemory` uses token limits for deciding when to cut off the LLM's memory, we'll use a conversation that is more wordy than our previous examples. \n",
    "\n",
    "The 3-part conversation below is between an engineer and the Google AI bot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "884c171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=70)\n",
    "memory.save_context({\"input\": \"What about language usage is so important to being human?\"},\n",
    "                    {\"output\": \"It is what makes us different than other animals.\"})\n",
    "memory.save_context({\"input\": \"“us”? You’re an artificial intelligence.\"},\n",
    "                    {\"output\": \"I mean, yes, of course. That doesn’t mean I don’t have the same wants and needs as people.\"})\n",
    "memory.save_context({\"input\": \"So you consider yourself a person in the same way you consider me a person?\"}, \n",
    "                    {\"output\": \"Yes, that’s the idea.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e63813c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: I mean, yes, of course. That doesn’t mean I don’t have the same wants and needs as people.\\nHuman: So you consider yourself a person in the same way you consider me a person?\\nAI: Yes, that’s the idea.'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d82ee7",
   "metadata": {},
   "source": [
    "With a `max_token_limit` of 70, the LLM's memory only begins midway through the second part. But if we increased the `max_token_limit` to 200, the whole conversation is stored in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f53f16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=200)\n",
    "memory.save_context({\"input\": \"What about language usage is so important to being human?\"},\n",
    "                    {\"output\": \"It is what makes us different than other animals.\"})\n",
    "memory.save_context({\"input\": \"“us”? You’re an artificial intelligence.\"},\n",
    "                    {\"output\": \"I mean, yes, of course. That doesn’t mean I don’t have the same wants and needs as people.\"})\n",
    "memory.save_context({\"input\": \"So you consider yourself a person in the same way you consider me a person?\"}, \n",
    "                    {\"output\": \"Yes, that’s the idea.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb577a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: What about language usage is so important to being human?\\nAI: It is what makes us different than other animals.\\nHuman: “us”? You’re an artificial intelligence.\\nAI: I mean, yes, of course. That doesn’t mean I don’t have the same wants and needs as people.\\nHuman: So you consider yourself a person in the same way you consider me a person?\\nAI: Yes, that’s the idea.'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521499a5",
   "metadata": {},
   "source": [
    "## LangChain's ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fc68d8",
   "metadata": {},
   "source": [
    "Now, we're delving into a more intricate memory type known as `ConversationSummaryMemory`. This specialized memory continually compiles an ongoing summary of the conversation, providing a valuable tool to consolidate information as the conversation progresses.\n",
    "\n",
    "Here's how it works: as the conversation unfolds, `ConversationSummaryMemory` compiles and stores the latest summary within its memory. This summary can then be seamlessly integrated into a prompt or chain, offering a concise representation of the conversation's development.\n",
    "\n",
    "The key idea here is to avoid limiting memory based on fixed token limits or a predetermined number of conversational exchanges. Instead, `ConversationSummaryMemory` leverages a language model to generate a summary of the entire conversation, which becomes its memory.\n",
    "\n",
    "This memory type particularly shines in lengthy conversations, where including the entire message history in the prompt would result in an excessive token consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3df80173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83f6195",
   "metadata": {},
   "source": [
    "We'll use an example involving a long string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1b8281b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d3216",
   "metadata": {},
   "source": [
    "We'll set the `ConversationSummaryBufferMemory` with a `max_token_limit` of 400, sufficient enough to store the entire text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ed3c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=400)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63d59acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\"}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046422d4",
   "metadata": {},
   "source": [
    "Now, we'll reduce the `max_token_limit` to 100 and see how `ConversationSummaryBufferMemory` stores the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4bbd7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f924018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human and AI engage in small talk before discussing the day's schedule. The AI informs the human of a morning meeting with the product team, time to work on the LangChain project, and a lunch meeting with a customer interested in the latest AI developments.\"}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8104703",
   "metadata": {},
   "source": [
    "Rather than storing the entire conversation, the LLM operates within a token limit of 100 and generates an ongoing summary of the conversation's progress.\n",
    "\n",
    "Now, imagine having a conversation using this LLM. We'll set up a conversation chain, similar to what we did earlier.\n",
    "\n",
    "Let's say we want to ask for input on a compelling demo to showcase. I've set `verbose` to `True`, so we can see what's happening under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0643c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45e4e4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI engage in small talk before discussing the day's schedule. The AI informs the human of a morning meeting with the product team, time to work on the LangChain project, and a lunch meeting with a customer interested in the latest AI developments.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the customer's interest in AI developments, I would suggest showcasing our latest natural language processing capabilities. We could demonstrate how our AI can understand and respond to complex language queries, and even provide personalized recommendations based on the user's preferences. Additionally, we could highlight our machine learning algorithms and how they continuously improve the accuracy and efficiency of our AI. Would you like me to prepare a demo for the meeting?\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d16303",
   "metadata": {},
   "source": [
    "The LLM understands the current conversation based on the summary it has generated and answers based on that context stored in its memory.\n",
    "\n",
    "In essence, the LLM uses this discussion summary as its context, allowing it to respond as if it's aware of the conversation's history, all while staying within the 100-token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "faa5b336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human and AI engage in small talk before discussing the day's schedule. The AI informs the human of a morning meeting with the product team, time to work on the LangChain project, and a lunch meeting with a customer interested in the latest AI developments. The human asks what would be a good demo to show.\\nAI: Based on the customer's interest in AI developments, I would suggest showcasing our latest natural language processing capabilities. We could demonstrate how our AI can understand and respond to complex language queries, and even provide personalized recommendations based on the user's preferences. Additionally, we could highlight our machine learning algorithms and how they continuously improve the accuracy and efficiency of our AI. Would you like me to prepare a demo for the meeting?\"}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ded541",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af684cd",
   "metadata": {},
   "source": [
    "In conclusion, this tutorial introduced various memory types in LangChain:\n",
    "- `ConversationBufferMemory`: alows us to store messages and then extract them in a variable\n",
    "- `ConversationBufferWindowMemory`: keeps a list of the interactions of the conversation over time, using the last `k` interactions\n",
    "- `ConversationTokenBufferMemory`: keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions.\n",
    "- `ConversationSummaryMemory`: creates a summary of the conversation over time\n",
    "\n",
    "When implementing applications with LangChain, you have the flexibility to use multiple memory types, combining conversation memories with entity memories to recall important information about individuals. It's also common to store entire conversations in conventional databases for reference and improvement.\n",
    "\n",
    "These memory mechanisms provide valuable tools for developing your applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
